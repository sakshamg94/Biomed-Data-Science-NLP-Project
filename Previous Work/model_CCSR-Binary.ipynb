{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 271 CCSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define imports\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "from numpy import asarray\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import time\n",
    "#\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in relevant Data Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the filepaths below correspond to your local copies of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "PATIENT_DATA_FILE = 'B220_SAA_v1.csv'\n",
    "CLEANED_LABELS_FILE = 'ICD_Label_Cleaned_Oct_25.csv'\n",
    "CODE_DESC_FILE = 'BIODS220_ICD_Dx_10_9_v7 - icd_dx_10_9_v7.csv'\n",
    "ICD10_to_CCSR = 'ICD10-CCSR.csv'\n",
    "LANG_MODEL = 'CCSR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_dict(file_path: str, key: int, value: int):\n",
    "    ret_dict = {}\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        data = csv.reader(csvfile, delimiter=',')\n",
    "        for row in data:\n",
    "            ret_dict[row[key]] = row[value]\n",
    "    print(\"Reading {} complete!\".format(file_path))\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_dict():\n",
    "    category_dict = {\n",
    "        'Circulatory': 0,\n",
    "        'Dermatologic': 4,\n",
    "        'Endocrine & Immune': 6,\n",
    "        'Gastrointestinal': 1,\n",
    "        'Genitourinary': 1, \n",
    "        'Hematologic': 4,\n",
    "        'Infectious': 6,\n",
    "        'Injury': 2,\n",
    "        'Injury & Poisoning': 2,\n",
    "        'Poisoning': 2,\n",
    "        'Musculoskeletal': 2,\n",
    "        'Neurologic': 3,\n",
    "        'Other': 4,\n",
    "        'Obstetric': 5,\n",
    "        'Neoplastic': 4,\n",
    "        'Psychiatric': 3,\n",
    "        'Respiratory': 0,\n",
    "        'Substance use': 2}\n",
    "    #use to_categorical()\n",
    "    return category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label2className_dict():\n",
    "    '''\n",
    "    Function to create reverse map of labels to category name\n",
    "    '''\n",
    "    className_dict = {\n",
    "        0:'Cardio-Resp',\n",
    "        1:'Abdominal',\n",
    "        2:'Injury/Subst/Poison', \n",
    "        3:'NeuroPsych',\n",
    "        4:'Other',\n",
    "        5:'Obstetric',\n",
    "        6:'Infection-Immune'\n",
    "    }\n",
    "    #use to_categorical()\n",
    "    return className_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ICD_Label_Cleaned_Oct_25.csv complete!\n",
      "Reading BIODS220_ICD_Dx_10_9_v7 - icd_dx_10_9_v7.csv complete!\n",
      "Reading ICD10-CCSR.csv complete!\n"
     ]
    }
   ],
   "source": [
    "# Create labels dict i.e. code -> label, i.e. A840 -> 'Neurologic'\n",
    "label_dict = read_csv_to_dict(CLEANED_LABELS_FILE, key=0, value=1)\n",
    "\n",
    "# Create descriptions dict i.e. code -> description\n",
    "codes_dict = read_csv_to_dict(CODE_DESC_FILE, key=0, value=2)\n",
    "\n",
    "# Create mapping from ICD10 code to CCSR category\n",
    "ccsr_dict = read_csv_to_dict(ICD10_to_CCSR, key=0, value=1)\n",
    "\n",
    "# Create dict for label to int\n",
    "category_dict = get_category_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create one-hot feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the table of ICD10 codes to CCSR categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ICD-10-CM Code</th>\n",
       "      <th>CCSR Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A000</td>\n",
       "      <td>DIG001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A000</td>\n",
       "      <td>INF003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A001</td>\n",
       "      <td>DIG001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A001</td>\n",
       "      <td>INF003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A009</td>\n",
       "      <td>DIG001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83452</th>\n",
       "      <td>Z9912</td>\n",
       "      <td>FAC012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83453</th>\n",
       "      <td>Z992</td>\n",
       "      <td>FAC025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83454</th>\n",
       "      <td>Z993</td>\n",
       "      <td>FAC025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83455</th>\n",
       "      <td>Z9981</td>\n",
       "      <td>FAC025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83456</th>\n",
       "      <td>Z9989</td>\n",
       "      <td>FAC025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83457 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ICD-10-CM Code CCSR Category\n",
       "0               A000        DIG001\n",
       "1               A000        INF003\n",
       "2               A001        DIG001\n",
       "3               A001        INF003\n",
       "4               A009        DIG001\n",
       "...              ...           ...\n",
       "83452          Z9912        FAC012\n",
       "83453           Z992        FAC025\n",
       "83454           Z993        FAC025\n",
       "83455          Z9981        FAC025\n",
       "83456          Z9989        FAC025\n",
       "\n",
       "[83457 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(ICD10_to_CCSR, usecols = ['ICD-10-CM Code', 'CCSR Category'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ICD-10-CM Code    73205\n",
       "CCSR Category       540\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have over 83,000 ICD-10 codes that are assigned to one of 540 categories, significantly reducing the dimensionality of the data! We're next going to one hot encode each ICD-10 code into a vector of 540 entries, each corresponding to a particular CCSR category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83457, 540)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccsr_one_hot = pd.get_dummies(df['CCSR Category']).to_numpy()\n",
    "ccsr_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICD codes may map to more than one CCSR Category. In this case, we want to ensure that the one-hot encoding includes multiple CCSR categories. To accomplish this, we will create a dictionary that maps an ICD code to the corresponding 540-vector encoding. We should have 73,205 keys in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd_to_ccsr = {} # Maps icd-10 code to ccsr category (which is 540-vector)\n",
    "\n",
    "for i in range(len(ccsr_one_hot)):\n",
    "    icd_code = df.iloc[i, 0] # 'A000'\n",
    "    encoding = ccsr_one_hot[i,:] # one-hot encoding of size (540,)\n",
    "    \n",
    "    try:\n",
    "        prev_encoding = icd_to_ccsr[icd_code]\n",
    "        # if a previous encoding already exists, \n",
    "        # then we concatenate it with the one-hot encoding of the current category\n",
    "        encoding = np.sum([prev_encoding, encoding], axis=0)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    icd_to_ccsr[icd_code] = encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we have one key per each unique ICD code. Thus, we are mapping every key to a (540,) one-hot encoding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(icd_to_ccsr.keys()) == df['ICD-10-CM Code'].nunique())\n",
    "ccsr_one_hot = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of reducing feature space, we are only including three features in our embedding. Future iterations of our embeddings will include more features, such as the patient's county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot(patient_data):\n",
    "    # Creates one-hot vectors\n",
    "    columns_to_one_hot = ['Sex','Race']\n",
    "    one_hot = pd.get_dummies(patient_data[columns_to_one_hot])\n",
    "    \n",
    "    ordinal_columns = ['Age']\n",
    "    one_hot = pd.concat([patient_data[ordinal_columns], one_hot], axis=1)\n",
    "    \n",
    "    # Normalize age\n",
    "    x = one_hot.Age.values.reshape(-1,1)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    one_hot.Age = x_scaled.astype(np.int16)\n",
    "    \n",
    "    return one_hot.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create one-hot ICD code encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an ICD code, we retrieve the one hot encoding (540x1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_to_embedding(code):\n",
    "    try:\n",
    "        embedding = icd_to_ccsr[code]\n",
    "    except:\n",
    "        print(code)\n",
    "        return np.zeros(540)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row of ICD Codes (i.e ['E839', 'SA920']), we retrieve the corresponding embeddings. For patient visits with n ICD Codes where n>1, we give 0.75 weight to the primary ICD code and 0.25/n-1 weight to the remaining (secondary) codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invalidCode(code):\n",
    "    numbers = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n",
    "    return code[0] in numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowToEncoding(row):\n",
    "    \"\"\"\n",
    "    input_row: A list of ICD10 codes\n",
    "    returns a 540x1 embedding\n",
    "    \"\"\"\n",
    "    patient_ICD_codes = [entry for entry in row[16:41] if entry != '']\n",
    "    n = len(patient_ICD_codes)\n",
    "    code = patient_ICD_codes[0]\n",
    "    if invalidCode(code):\n",
    "        print(\"Invalid primary code: \", code)\n",
    "        return\n",
    "    # Primary ICD code:\n",
    "    primary_embedding = code_to_embedding(code)\n",
    "    if n < 2: return primary_embedding\n",
    "    \n",
    "    # Subsequent ICD codes:\n",
    "    secondary_embedding = None\n",
    "    for i in range(1, n):\n",
    "        code = patient_ICD_codes[i]\n",
    "        if invalidCode(code):\n",
    "            print(\"Invalid secondary code: \", code)\n",
    "            continue\n",
    "        curr_embed = code_to_embedding(code)\n",
    "        if secondary_embedding is None:\n",
    "            secondary_embedding = curr_embed\n",
    "        else:\n",
    "            secondary_embedding = np.sum([secondary_embedding, curr_embed], axis=0)\n",
    "    \n",
    "    \n",
    "    return np.sum([primary_embedding, secondary_embedding], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** Since a patient may have more than one ICD-10 code attributed to their visit to the emergency room, two or more codes may share the same CCSR category. Thus, when we take the sum of the CCSR one-hot encodings, we may end up with numbers greater than 1. Since we ultimately want the 540-vector encoding of the patient's visit to be a binary representation of the CCSR categories, numbers greater than 1 should be reduced to 1. This case will only occur when a patient comes in with more than one ICD_10 code that shares the same CCSR category, in which case we will be taking the sum of two identical one-hot encodings, producing something similar to [0, ..., 2, ...0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the patient visit csv file again and create embeddings for each visit as we read the file. For each visit, we have already computed the one-hot vector encoding for categorical and ordinal variables. Here, we combine those encodings with ICD_10 code embeddings. To reduce model complexity, we limit the number of patients (not total visits) for which to create embeddings. Here, we are working under the assumption that all visits for a patient appear one after the other in the csv file, thus. Takes a few mins to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid secondary code:  129\n",
      "Reading:  50000  patients\n",
      "Invalid secondary code:  1510\n",
      "Reading:  100000  patients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(628402, 542)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_NUM_PATIENTS = 0.1e6 #changed for debugging  \n",
    "\n",
    "def get_visit_embedding(input_file_path: str): # input_file_path = PATIENT_DATA_FILE\n",
    "    start_time = time.time()\n",
    "    visit_encodings = []\n",
    "    patient_visit = []\n",
    "    with open(input_file_path, newline='') as csvfile:\n",
    "        data = csv.reader(csvfile, delimiter=',')\n",
    "        num_patients = 0\n",
    "        first_row = True\n",
    "        curr_patient_id = None\n",
    "        \n",
    "        for row in data:\n",
    "            \n",
    "            if first_row:\n",
    "                first_row = False\n",
    "                continue\n",
    "\n",
    "            new_patient_id = row[0]\n",
    "            \n",
    "            # Used to limit num patients, reducing model space\n",
    "            if curr_patient_id != new_patient_id:\n",
    "                \n",
    "                # If we have enough patients, break\n",
    "                if num_patients >= TOTAL_NUM_PATIENTS:\n",
    "                    break\n",
    "                else:\n",
    "                    curr_patient_id = new_patient_id\n",
    "                    num_patients += 1\n",
    "                    if num_patients % 50000 == 0:\n",
    "                        print(\"Reading: \", num_patients, \" patients\")\n",
    "\n",
    "            # Convert list of ICD-10 codes to 540-vector one-hot encoding\n",
    "            code_embedding = rowToEncoding(row).astype(np.int16)\n",
    "            visit_encodings.append(code_embedding)\n",
    "            patient_visit.append([int(row[0]), int(row[1])]) # Store patient id and visit id\n",
    "    \n",
    "    # Reduce entries to be binary (0 or 1)\n",
    "    code_dset = np.array(visit_encodings)\n",
    "    min_arr = np.ones((len(code_dset), len(code_dset[0])))\n",
    "    assert(min_arr.shape == code_dset.shape)\n",
    "    code_dset = np.minimum(code_dset, min_arr)\n",
    "\n",
    "    # Combine patient id, visit id and one-hot encoding\n",
    "    visit_arr = np.array(patient_visit)\n",
    "    result = np.hstack((visit_arr, code_dset)) # patient id, visit id, one-hot encoding vector\n",
    "    return result\n",
    "\n",
    "code_dset = get_visit_embedding(PATIENT_DATA_FILE)\n",
    "num_visits = len(code_dset)\n",
    "code_dset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in patient_data took 1.1798033714294434\n",
      "Creating one-hot encodings took 0.3289210796356201\n",
      "(628402, 11)\n"
     ]
    }
   ],
   "source": [
    "# Read in patient_data - takes about a minute\n",
    "patient_read_start = time.time()\n",
    "patient_data = pd.read_csv(PATIENT_DATA_FILE, dtype=str, usecols=['ID', 'Sex','Race','Age', 'Date'], nrows = num_visits)\n",
    "print(\"Reading in patient_data took {}\".format(time.time() - patient_read_start))\n",
    "\n",
    "# One-hot encode visit features\n",
    "one_hot_start = time.time()\n",
    "one_hot_features = create_one_hot(patient_data)\n",
    "print(\"Creating one-hot encodings took {}\".format(time.time() - one_hot_start))\n",
    "print(one_hot_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Concatenate Feature vector with ICD-10 code encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dset = np.hstack((code_dset, one_hot_features[:num_visits,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(628402, 553)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5A: Get labels (slow method sorted by pt ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create labels, we convert the 'Date' column into pandas datetime. We then group by patient 'ID' and determine the difference between the next row to the previous. This means if a row value in y_diff is 30 then the next visit is 30 days from the previous. We then set the label to 0 if visit is > 30 days, 1 if it is <= 30 days, or 2 if there is no next visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_labels(patient_data):\n",
    "\n",
    "    patient_data['Date'] = pd.to_datetime(patient_data['Date'])  #convert date column to date-time type\n",
    "    y_diff = abs(patient_data.groupby(['ID'])['Date'].diff(periods=-1))\n",
    "    y_diff = y_diff.dt.days\n",
    "    y_label = []\n",
    "    for i in y_diff:\n",
    "        if pd.isnull(i):\n",
    "            y_label.append(2)\n",
    "        else:\n",
    "            if i <= 30:\n",
    "                y_label.append(1)\n",
    "            else:\n",
    "                y_label.append(0)\n",
    "    return y_label, len(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating labels took 14.187441110610962\n"
     ]
    }
   ],
   "source": [
    "# Get y_dset\n",
    "label_start = time.time()\n",
    "y_dset, num_labels = get_y_labels(patient_data)\n",
    "print(\"Creating labels took {}\".format(time.time() - label_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures the number of labels corresponds to the number of patient visits\n",
    "assert(num_labels == num_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(628402, 554)\n"
     ]
    }
   ],
   "source": [
    "new_dset = np.hstack([x_dset, np.array(y_dset).reshape(len(y_dset),1)])\n",
    "print(new_dset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_dset, y_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dset = new_dset[new_dset[:,-1]!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(528402, 554)\n"
     ]
    }
   ],
   "source": [
    "print(new_dset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5B: Get labels (fast method not sorted by pt ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create labels, we convert the 'Date' column into pandas datetime. We then check the difference between rows without sorting. This means if a row is 30 then the next visit is 30 days from the previous. We then set the label to 0 if visit is > 30 days, 1 if it is less than 30 days. The nth visit per patient will be incorrect as it will be using the 1st visit from the next patient to determine the time difference. If we are going to use this we will have to do some processing when we split data by patient and assert that the label for the last visit is 'Na' or '2' or some other marker to show that there is no next visit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_y_labels(patient_data):\n",
    "\n",
    "#     patient_data['Date'] = pd.to_datetime(patient_data['Date'])  #convert date column to date-time type\n",
    "#     y_diff = abs(patient_data['Date'].diff(periods=-1))\n",
    "#     y_diff = y_diff.dt.days\n",
    "#     y_label = []\n",
    "#     for i in y_diff:\n",
    "#         if i <= 30:\n",
    "#             y_label.append(1)\n",
    "#         else:\n",
    "#             y_label.append(0)\n",
    "#     return y_label, len(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating labels took 0.2488718032836914\n"
     ]
    }
   ],
   "source": [
    "# # Get y_dset\n",
    "# label_start = time.time()\n",
    "# y_dset, num_labels = get_y_labels(patient_data)\n",
    "# print(\"Creating labels took {}\".format(time.time() - label_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensures the number of labels corresponds to the number of patient visits\n",
    "# assert(num_labels == num_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save embeddings and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings took: 119.45295143127441\n"
     ]
    }
   ],
   "source": [
    "# filename = 'embeddings_{}.pickle'.format(LANG_MODEL)\n",
    "\n",
    "# start_time = time.time()\n",
    "# pickle_out = open(filename, 'wb')\n",
    "# pickle.dump((x_dset, y_dset), pickle_out, protocol=4)\n",
    "# pickle_out.close()\n",
    "# print(\"Saving embeddings took: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings took: 110.1644721031189\n"
     ]
    }
   ],
   "source": [
    "filename = 'embeddings_{}.pickle'.format(LANG_MODEL)\n",
    "start_time = time.time()\n",
    "pickle_in = open(filename, 'rb')\n",
    "embedding_dset, y_dset = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "print(\"Saving embeddings took: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Store patient_id, visit_id, embedding_vec, label in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "collated_data = new_dset #np.hstack((x_dset, y_dset.reshape(y_dset.shape[0],1))) # -- last col is label column, first 2 are patient an visit ids\n",
    "cols =  ['patient_id', 'visit_id'] + ['embed_vec'+ str(i) for i in range(new_dset.shape[1]-3)] + ['label']\n",
    "df = pd.DataFrame(data = collated_data, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528402, 554)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear variables\n",
    "collated_data = None\n",
    "new_dset = None\n",
    "x_dset = None\n",
    "y_dset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['patient_id'] = df['patient_id'].astype('int64')\n",
    "df['visit_id'] = df['visit_id'].astype('int64')\n",
    "for col in cols[2:-1]:\n",
    "    df[col] = df[col].astype(str).astype(np.float32)\n",
    "\n",
    "df['label'] = df['label'].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_id        int64\n",
       "visit_id          int64\n",
       "embed_vec0      float32\n",
       "embed_vec1      float32\n",
       "embed_vec2      float32\n",
       "                 ...   \n",
       "embed_vec547    float32\n",
       "embed_vec548    float32\n",
       "embed_vec549    float32\n",
       "embed_vec550    float32\n",
       "label             int16\n",
       "Length: 554, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment out the pickle dump/load sections below based on need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'patient_dataframe_03_24_2021.pkl'\n",
    "pickle_out = open(filename, 'wb')\n",
    "pickle.dump(df, pickle_out, protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'patient_dataframe.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-a3df26cf9584>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'patient_dataframe.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpickle_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpickle_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'patient_dataframe.pkl'"
     ]
    }
   ],
   "source": [
    "# filename = 'patient_dataframe.pkl'\n",
    "# pickle_in = open(filename, 'rb')\n",
    "# df = pickle.load(pickle_in)\n",
    "# pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>visit_id</th>\n",
       "      <th>embed_vec0</th>\n",
       "      <th>embed_vec1</th>\n",
       "      <th>embed_vec2</th>\n",
       "      <th>embed_vec3</th>\n",
       "      <th>embed_vec4</th>\n",
       "      <th>embed_vec5</th>\n",
       "      <th>embed_vec6</th>\n",
       "      <th>embed_vec7</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_vec542</th>\n",
       "      <th>embed_vec543</th>\n",
       "      <th>embed_vec544</th>\n",
       "      <th>embed_vec545</th>\n",
       "      <th>embed_vec546</th>\n",
       "      <th>embed_vec547</th>\n",
       "      <th>embed_vec548</th>\n",
       "      <th>embed_vec549</th>\n",
       "      <th>embed_vec550</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 554 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  visit_id  embed_vec0  embed_vec1  embed_vec2  embed_vec3  \\\n",
       "0           1         1         0.0         0.0         0.0         0.0   \n",
       "1           1         2         0.0         0.0         0.0         0.0   \n",
       "2           1         3         0.0         0.0         0.0         0.0   \n",
       "3           2         1         0.0         0.0         0.0         0.0   \n",
       "4           2         2         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   embed_vec4  embed_vec5  embed_vec6  embed_vec7  ...  embed_vec542  \\\n",
       "0         0.0         0.0         0.0         0.0  ...           0.0   \n",
       "1         0.0         0.0         0.0         0.0  ...           0.0   \n",
       "2         0.0         0.0         0.0         0.0  ...           0.0   \n",
       "3         0.0         0.0         0.0         0.0  ...           1.0   \n",
       "4         0.0         0.0         0.0         0.0  ...           1.0   \n",
       "\n",
       "   embed_vec543  embed_vec544  embed_vec545  embed_vec546  embed_vec547  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0   \n",
       "1           0.0           0.0           0.0           0.0           0.0   \n",
       "2           0.0           0.0           0.0           0.0           0.0   \n",
       "3           0.0           0.0           0.0           1.0           0.0   \n",
       "4           0.0           0.0           0.0           1.0           0.0   \n",
       "\n",
       "   embed_vec548  embed_vec549  embed_vec550  label  \n",
       "0           0.0           0.0           1.0      0  \n",
       "1           0.0           0.0           1.0      1  \n",
       "2           0.0           0.0           1.0      0  \n",
       "3           0.0           0.0           0.0      1  \n",
       "4           0.0           0.0           0.0      1  \n",
       "\n",
       "[5 rows x 554 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create the binary classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(528402, 551) 528402\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "properties = list(df.columns.values)\n",
    "X = df[properties[2:-1]]\n",
    "y = df['label']\n",
    "print(X.shape, len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(X.shape[1],)),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualiza the data with tSNE/PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "N = 10000\n",
    "# For reproducability of the results\n",
    "np.random.seed(42)\n",
    "rndperm = np.random.permutation(df.shape[0])\n",
    "feat_cols = list(np.arange(0,df.shape[1]-1)) # replace 0 with 11 for removing demographics\n",
    "print(feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.18 GiB for an array with shape (553, 528402) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-600481a4a3dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpca_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeat_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pca-one'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mvalues\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5341\u001b[0m         \"\"\"\n\u001b[0;32m   5342\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5345\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mas_array\u001b[1;34m(self, transpose, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m    851\u001b[0m                     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m             \u001b[1;31m# The underlying data was copied within _interleave\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m    880\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"object\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[0mitemmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.18 GiB for an array with shape (553, 528402) and data type float64"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "pca_result = pca.fit_transform(df.iloc[:,feat_cols].values)\n",
    "\n",
    "df['pca-one'] = pca_result[:,0]\n",
    "df['pca-two'] = pca_result[:,1] \n",
    "df['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    hue=\"label\",\n",
    "    palette=sns.color_palette(\"hls\", NUM_CLASSES),\n",
    "    data=df.loc[rndperm,:],\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")\n",
    "plt.show()\n",
    "# plt.savefig('PCA.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
