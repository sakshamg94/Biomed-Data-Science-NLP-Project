{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 271 Project Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-60497a9e8e80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Define imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import asarray\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import gensim.downloader as api\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read in relevant Data Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the files below are in the same directory as your jupyter file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "PATIENT_DATA_FILE = 'B220_SAA_v1.csv'\n",
    "CLEANED_LABELS_FILE = 'ICD_Label_Cleaned_Oct_25.csv'\n",
    "CODE_DESC_FILE = 'BIODS220_ICD_Dx_10_9_v7 - icd_dx_10_9_v7.csv'\n",
    "# May want to consider: glove-wiki-gigaword-100 in the future\n",
    "# See more here: https://github.com/RaRe-Technologies/gensim-data\n",
    "LANG_MODEL = 'word2vec-google-news-300'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_dict(file_path: str, key: int, value: int):\n",
    "    ret_dict = {}\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        data = csv.reader(csvfile, delimiter=',')\n",
    "        for row in data:\n",
    "            ret_dict[row[key]] = row[value]\n",
    "    print(\"Reading {} complete!\".format(file_path))\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_dict():\n",
    "    category_dict = {\n",
    "        'Circulatory': 1,\n",
    "        'Dermatologic': 2,\n",
    "        'Endocrine & Immune': 3,\n",
    "        'Gastrointestinal': 4,\n",
    "        'Genitourinary': 5, \n",
    "        'Hematologic': 6,\n",
    "        'Infectious': 7,\n",
    "        'Injury & Poisoning': 8,\n",
    "        'Musculoskeletal': 9,\n",
    "        'Neurologic': 10,\n",
    "        'Other': 11,\n",
    "        'Obstetric': 12,\n",
    "        'Neoplastic': 13,\n",
    "        'Psychiatric': 14,\n",
    "        'Respiratory': 15,\n",
    "        'Substance use': 16}\n",
    "    #use to_categorical()\n",
    "    return category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading word2vec-google-news-300 complete!\n",
      "Reading ICD_Label_Cleaned_Oct_25.csv complete!\n",
      "Reading BIODS220_ICD_Dx_10_9_v7 - icd_dx_10_9_v7.csv complete!\n"
     ]
    }
   ],
   "source": [
    "# Import word to vec model\n",
    "wv = api.load(LANG_MODEL)\n",
    "print(\"Reading {} complete!\".format(LANG_MODEL))\n",
    "\n",
    "# Create labels dict i.e. code -> label, i.e. A840 -> 'Neurologic'\n",
    "label_dict = read_csv_to_dict(CLEANED_LABELS_FILE, key=0, value=1)\n",
    "\n",
    "# Create descriptions dict i.e. code -> description\n",
    "codes_dict = read_csv_to_dict(CODE_DESC_FILE, key=0, value=2)\n",
    "\n",
    "# Create dict for label to int\n",
    "category_dict = get_category_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create one-hot feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of reducing feature space, we are only including three features in our embedding. Future iterations of our embeddings will include more features, such as the patient's county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot(patient_data):\n",
    "    # Creates one-hot vectors\n",
    "    columns_to_one_hot = ['Sex','Race']\n",
    "    one_hot = pd.get_dummies(patient_data[columns_to_one_hot])\n",
    "    \n",
    "    ordinal_columns = ['Age']\n",
    "    one_hot = pd.concat([patient_data[ordinal_columns], one_hot], axis=1)\n",
    "    \n",
    "    # Normalize age\n",
    "    x = one_hot.Age.values.reshape(-1,1)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    one_hot.Age = x_scaled\n",
    "    \n",
    "    return one_hot.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in patient_data took 44.95810008049011\n",
      "Creating one-hot encodings took 11.02394723892212\n",
      "(27977932, 11)\n"
     ]
    }
   ],
   "source": [
    "# Read in patient_data\n",
    "patient_read_start = time.time()\n",
    "patient_data = pd.read_csv(PATIENT_DATA_FILE, dtype=str, usecols=['Sex','Race','Age','Dx10_prin'])\n",
    "print(\"Reading in patient_data took {}\".format(time.time() - patient_read_start))\n",
    "\n",
    "# One-hot encode visit features\n",
    "one_hot_start = time.time()\n",
    "one_hot = create_one_hot(patient_data)\n",
    "print(\"Creating one-hot encodings took {}\".format(time.time() - one_hot_start))\n",
    "print(one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (num_patients, 311)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Embed ICD Codes + concat with one-hot vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sentence, retrieve the pre-trained word2vec embedding for each word in the sentence and return the mean of the embeddings. If the sentence has no words that exist in the word2vec model, we return None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentence, split_tokens): \n",
    "    # Revisit: since medical terms may not be present, although they are important to include\n",
    "    word_embeddings = []\n",
    "    for word in re.split(split_tokens, sentence):\n",
    "        try:\n",
    "            word_embeddings.append(wv[word.lower()])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if len(word_embeddings)>0:\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sentence and label, return the embedding of a sentence. If there is no embedding for the sentence then return the embedding for the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_embedding(sentence, label):\n",
    "    \"\"\"\n",
    "    Sentence: A description of an ICD10 code, usually 1-5 words\n",
    "    Category: The class of the diagnosis, one of 17 options\n",
    "    \"\"\"\n",
    "    embedding = embed_sentence(sentence, split_tokens='\\s|(?<!\\d)[:;,.\\(\\)\\[\\]-](?!\\d)')\n",
    "    \n",
    "    # If we can't create embedding with ICD10 codes, we use category\n",
    "    if embedding is not None:\n",
    "        return embedding\n",
    "    else:\n",
    "        return embed_sentence(label, split_tokens='\\s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an ICD code, return the corresponding description. Here, we attempt to take handle mistyped data. Some codes are missing 1-2 characters. Sometimes, adding a 0 or removing the last character in a code will fix typos, however this strategy is prone to error and not guarunteed to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desc(code):\n",
    "    try:\n",
    "        desc = codes_dict[code]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            new_code = code + '0'\n",
    "            desc = codes_dict[new_code]\n",
    "        except:\n",
    "            try:\n",
    "                new_code = code[:-1]\n",
    "                desc = codes_dict[new_code]\n",
    "            except:\n",
    "                desc = 'Other'\n",
    "    try:\n",
    "        label = labels_dict[code]\n",
    "    except:\n",
    "        label = 'Other'\n",
    "    return desc, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an ICD code, we retrieve the embedding (300x1). We use dynamic programming to save embeddings. Often, only the first three characters of an ICD code are enough to determine the general diagnosis. Longer codes will only have slightly different descriptions (if at all). Thus, to reduce computational complexity, we store the first three characters of ICD codes. Future ICD codes that share the same first three characters will automatically use the same embedding, regardless of any remaining characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_dict = {}\n",
    "\n",
    "def code_to_embedding(code):\n",
    "    try:\n",
    "        embedding = descriptions_dict[str(code[:3])]\n",
    "    except:\n",
    "        desc, category = get_desc(code)\n",
    "        embedding = get_w2v_embedding(desc, category)\n",
    "        if desc is not 'Other':\n",
    "            descriptions_dict[str(code[:3])] = embedding\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row of ICD Codes (i.e ['E839', 'SA920']), we retrieve the corresponding embeddings. For patient visits with n ICD Codes where n>1, we give 0.75 weight to the primary ICD code and 0.25/n-1 weight to the remaining (secondary) codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMARY_WEIGHT = 0.75\n",
    "SECONDARY_WEIGHT = 0.25\n",
    "\n",
    "def row_to_embedding(input_row):\n",
    "    \"\"\"\n",
    "    input_row: A list of ICD10 codes\n",
    "    returns a 300x1 embedding\n",
    "    \"\"\"\n",
    "    n = len(input_row)\n",
    "    code = input_row[0]\n",
    "    # Primary ICD code:\n",
    "    primary_embedding = code_to_embedding(code)\n",
    "    if n < 2: return primary_embedding\n",
    "    \n",
    "    # Subsequent ICD codes:\n",
    "    secondary_embedding = None\n",
    "    for i in range(1, n):\n",
    "        code = input_row[i]\n",
    "        curr_embed = code_to_embedding(code)\n",
    "        if secondary_embedding is None:\n",
    "            secondary_embedding = curr_embed\n",
    "        else:\n",
    "            secondary_embedding = np.sum([secondary_embedding, curr_embed], axis=0)\n",
    "    \n",
    "    \n",
    "    return np.sum([primary_embedding * PRIMARY_WEIGHT, secondary_embedding * (SECONDARY_WEIGHT/n-1)], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the patient visit csv file again and create embeddings for each visit as we read the file. For each visit, we have already computed the one-hot vector encoding for categorical and ordinal variables. Here, we combine those encodings with ICD_10 code embeddings. To reduce model complexity, we limit the number of patients (not total visits) for which to create embeddings. Here, we are working under the assumption that all visits for a patient appear one after the other in the csv file, thus, we can stop reading from the file once `patient id > num_patients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 250000 visit embeddings in 15.989832162857056\n",
      "Completed 500000 visit embeddings in 31.793057918548584\n",
      "Completed 750000 visit embeddings in 46.7544960975647\n",
      "Completed 1000000 visit embeddings in 61.467241287231445\n",
      "Completed 1250000 visit embeddings in 76.34133696556091\n",
      "Completed 1500000 visit embeddings in 91.55047106742859\n",
      "Completed 1750000 visit embeddings in 106.88795018196106\n",
      "Completed 2000000 visit embeddings in 122.51593518257141\n",
      "Shape of embeddings: (2118142,311)\n"
     ]
    }
   ],
   "source": [
    "NUM_PATIENTS = 1e6\n",
    "\n",
    "def get_visit_embedding(input_file_path: str):\n",
    "    start_time = time.time()\n",
    "    embeddings = []\n",
    "    patient_visit = []\n",
    "    max_len = 0\n",
    "#     patient_visit = {\n",
    "#         1 : [[1, embed],[4, embed],[2,embed]],\n",
    "#         2 : [[embed],[embed],....],\n",
    "#         .\n",
    "#         .\n",
    "#         .\n",
    "#     }\n",
    "    with open(input_file_path, newline='') as csvfile:\n",
    "        data = csv.reader(csvfile, delimiter=',')\n",
    "        count = 0\n",
    "        for row in data: # <- top to bottom\n",
    "            if count == 0: # Skip the first row\n",
    "                count += 1\n",
    "                continue\n",
    "            \n",
    "            if int(row[0]) > NUM_PATIENTS: # Used to limit num patients, reducing model space\n",
    "                break\n",
    "                \n",
    "            ICD_code_embeddings = row_to_embedding([entry for entry in row[16:41] if entry is not ''])\n",
    "            \n",
    "            # Combine one-hot with w2v embeddings\n",
    "            visit_embedding = np.concatenate((one_hot[count-1, :], ICD_code_embeddings), axis=0)\n",
    "            embeddings.append(visit_embedding)\n",
    "            patient_visit.append([row[0], row[1]]) # store patient and visit info\n",
    "            \n",
    "            # Tracking progress\n",
    "            if count % 250000 == 0:\n",
    "                print(\"Completed {} visit embeddings in {}\".format(count, (time.time() - start_time)))\n",
    "            count +=1\n",
    "            \n",
    "    print(\"Shape of embeddings: ({},{})\".format(len(embeddings), len(embeddings[0])))\n",
    "    print(\"Shape of patient_visit: ({},{})\".format(len(patient_visit), len(patient_visit[0])))\n",
    "    embedding_vecs = np.array(embeddings)\n",
    "    result = np.hstack((np.array(patient_visits),embedding_vecs)) # patient id, visit id, embedding vector\n",
    "    return result, count-1\n",
    "\n",
    "embedding_dset, num_visits = get_visit_embedding(PATIENT_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 4) (7, 2)\n",
      "(7, 6)\n",
      "[[0.24997387 0.47208521 0.44250465 0.15573033]\n",
      " [0.97051585 0.91525352 0.67683293 0.94238081]\n",
      " [0.24690088 0.33114635 0.21995855 0.20040291]\n",
      " [0.11115457 0.25069068 0.46031599 0.03313348]\n",
      " [0.81267051 0.80007031 0.29333346 0.01576718]\n",
      " [0.68358734 0.35329325 0.82702192 0.64831648]\n",
      " [0.32509275 0.49822325 0.85422819 0.99266063]]\n",
      "[[1.         2.         0.24997387 0.47208521 0.44250465 0.15573033]\n",
      " [1.         2.         0.97051585 0.91525352 0.67683293 0.94238081]\n",
      " [1.         2.         0.24690088 0.33114635 0.21995855 0.20040291]\n",
      " [1.         2.         0.11115457 0.25069068 0.46031599 0.03313348]\n",
      " [1.         2.         0.81267051 0.80007031 0.29333346 0.01576718]\n",
      " [1.         2.         0.68358734 0.35329325 0.82702192 0.64831648]\n",
      " [1.         2.         0.32509275 0.49822325 0.85422819 0.99266063]]\n",
      "(3, 2)\n",
      "(5, 1)\n",
      "['patient_id', 'visit_id', 'embed_vec0', 'embed_vec1', 'embed_vec2', 'embed_vec3', 'label']\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# a = np.random.rand(7,4)\n",
    "# b = np.ones((7,2))\n",
    "# print(a.shape, b.shape)\n",
    "# b[:,1]= 2\n",
    "# res = np.hstack((b,a))\n",
    "# print(res.shape)\n",
    "# print(a)\n",
    "# print(res)\n",
    "# c = [[1,2],[3,4],[3,4]]\n",
    "# print(np.array(c).shape)\n",
    "# d = np.array(list([1,2,3,4,5]))\n",
    "# print(d.reshape(d.shape[0],1).shape)\n",
    "# cols =  ['patient_id', 'visit_id'] + ['embed_vec'+ str(i) for i in range(res.shape[1]-2)] + ['label']\n",
    "# print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create labels, we copy the primary diagnosis codes (`DX10_prin` column) into its own column named `Label`. For each code, we use the `label_dict` to retrive the corresponding category of the ICD code. Then we use the `category_dict` to retieve a number to represent the label. We limit the number of visits to the same size as the number of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Dx10_prin_to_label(DX_10_code):\n",
    "    try:\n",
    "        category = label_dict[DX_10_code] # i.e. A065 -> Infectious\n",
    "    except KeyError:\n",
    "        category = 'Other'\n",
    "    return category_dict[category] # i.e. Infectious -> 7 \n",
    "\n",
    "def get_y_labels(patient_data):\n",
    "    # Copy Dx10 code into new column named Label\n",
    "    patient_data['Label'] = patient_data.Dx10_prin\n",
    "\n",
    "    # Apply function to Label column to convert Dx10_prin code to category\n",
    "    patient_data.Label = patient_data.Label.apply(convert_Dx10_prin_to_label)\n",
    "    \n",
    "    # Saves only the top num_visits, since that's how many embeddings there are\n",
    "    y_list = patient_data.Label.iloc[:num_visits].to_list()\n",
    "    return np.array(y_list), len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating labels took 12.619440793991089\n"
     ]
    }
   ],
   "source": [
    "# Get y_dset\n",
    "label_start = time.time()\n",
    "y_dset, num_labels = get_y_labels(patient_data)\n",
    "print(\"Creating labels took {}\".format(time.time() - label_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures the number of labels corresponds to the number of patient visits\n",
    "assert(num_labels == num_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save embeddings and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'embeddings_{}.pickle'.format(LANG_MODEL)\n",
    "\n",
    "# start_time = time.time()\n",
    "# pickle_out = open(filename, 'wb')\n",
    "# pickle.dump((embedding_dset, y_dset), pickle_out, protocol=4)\n",
    "# pickle_out.close\n",
    "# print(\"Saving embeddings took: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Store patient_id, visit_id, embedding_vec, label in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collated_data = np.vstack((embedding_dset, y_dset.reshape(y_dset.shape[0],1)))\n",
    "cols =  ['patient_id', 'visit_id'] + ['embed_vec'+ str(i) for i in range(embedding_dset.shape[1]-2)] + ['label']\n",
    "df = pd.DataFrame(data = collated_data, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1]]\n",
      "B\n",
      "[[8, 4]]\n",
      "D\n",
      "[[4, 3]]\n",
      "F\n",
      "[[7, 2]]\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.DataFrame({\n",
    "\n",
    "#     'patient_id': ['A', 'A', 'B', 'B', 'D', 'C'],\n",
    "\n",
    "#     'visit_id': [2, 1, 9, 8, 7, 4],\n",
    "\n",
    "#     'col3': [0, 1, 9, 4, 2, 3],\n",
    "\n",
    "#     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n",
    "\n",
    "# })\n",
    "# df = df.sort_values(['patient_id', 'visit_id'])\n",
    "# gf = df.groupby('patient_id')\n",
    "# for patient in df['patient_id'].unique():\n",
    "#     curr_patient = gf.get_group(patient)\n",
    "#     num_visits  = curr_patient.shape[0]\n",
    "# #     print(curr_patient)\n",
    "#     print([list(curr_patient.iloc[0,1:-1].values)])\n",
    "#     print(curr_patient.iloc[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(['patient_id', 'visit_id'])\n",
    "gf = df.groupby('patient_id')\n",
    "data_X = []\n",
    "data_Y = []\n",
    "for patient in df['patient_id'].unique():\n",
    "    curr_patient = gf.get_group(patient)\n",
    "    num_visits  = curr_patient.shape[0]\n",
    "    if num_visits < 3:\n",
    "        continue\n",
    "    elif num_visits == 3:\n",
    "        visit_0 = list(curr_patient.iloc[0,2:-1].values)\n",
    "        visit_1 = list(curr_patient.iloc[1,2:-1].values)\n",
    "        data_X.append([visit_0, visit_1])\n",
    "        data_Y.append(curr_patient.iloc[2,-1])\n",
    "    else:\n",
    "        for i in range(0, num_visits-2):\n",
    "            visit_0 = list(curr_patient.iloc[i,2:-1].values)\n",
    "            visit_1 = list(curr_patient.iloc[i+1,2:-1].values)\n",
    "            data_X.append([visit_0, visit_1])\n",
    "            data_Y.append(curr_patient.iloc[i+2,-1])\n",
    "\n",
    "data_X = np.array(data_X)\n",
    "# data_X= data_X.reshape()\n",
    "print(data_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "np.random.shuffle() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
