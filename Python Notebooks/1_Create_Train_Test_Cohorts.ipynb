{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "resident-barrier",
   "metadata": {},
   "source": [
    "# Create Train and Test Cohorts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-parade",
   "metadata": {},
   "source": [
    "## Load libraries and enable Pyspark and PyArrow to allow for distributed processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "august-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "potential-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import collections\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "casual-creator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'35G'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the session\n",
    "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '35G')\n",
    "        .set('spark.driver.memory', '35G')\n",
    "        .set('spark.driver.maxResultSize', '35G'))\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "# arrow enabling is what makes the conversion from pandas to spark dataframe really fast\n",
    "sc._conf.get('spark.driver.memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-sauce",
   "metadata": {},
   "source": [
    "## Read in raw data and select for diagnosis code columns (16-41) and demographic information (0-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faced-singapore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv took: 5.038235664367676 seconds\n"
     ]
    }
   ],
   "source": [
    "dirPath = '/home/ubuntu/BioMedProject/Data/'\n",
    "patientDataPath = dirPath + 'patientData.csv' # file was renamed from 'B220_SAA_v1.csv'\n",
    "\n",
    "begin = time.time()\n",
    "df = spark.read.load(patientDataPath, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "df = df.select(df.columns[0:8] + df.columns[16:41])\n",
    "df = df.drop('Type')\n",
    "end = time.time()\n",
    "print(\"Reading csv took: {} seconds\".format(end - begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-chorus",
   "metadata": {},
   "source": [
    "## Create 30-day readmission labels based on immediate future visit for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "naughty-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell took 0.05290389060974121\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "begin = time.time()\n",
    "my_window = Window.partitionBy().orderBy(\"ID\")\n",
    "\n",
    "df = df.withColumn(\"next_Date\", lead(df.Date).over(my_window))\n",
    "df = df.withColumn(\"next_Id\", lead(df.ID).over(my_window))\n",
    "df = df.withColumn(\"decFirst\", lit(\"2018-12-01\"))\n",
    "df = df.withColumn(\"daysUntilDecFirst\", datediff(col(\"decFirst\"), col(\"Date\")).alias(\"finalVisitDiff\"))\n",
    "df = df.withColumn(\"datediff\", datediff(col(\"next_Date\"), col(\"Date\")).alias(\"datediff\"))\n",
    "end = time.time()\n",
    "print(\"Cell took {}\".format(end - begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "israeli-collins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell took 0.02687835693359375\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "df = df.withColumn(\"label\", when((f.col(\"ID\") != f.col(\"next_Id\")) & (f.col(\"daysUntilDecFirst\") >= 0), 0)\\\n",
    "                   .when((f.col(\"ID\") != f.col(\"next_Id\")) & (f.col(\"daysUntilDecFirst\") < 0), 2)\\\n",
    "                   .when(col(\"datediff\") > 30, 0)\\\n",
    "                   .when(col(\"datediff\") <= 30, 1)\\\n",
    "                   .otherwise(2))\n",
    "end = time.time()\n",
    "print(\"Cell took {}\".format(end - begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aboriginal-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(f.col(\"label\") != 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "romance-series",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of visits: 27502638\n"
     ]
    }
   ],
   "source": [
    "total_count = df.count()\n",
    "print(\"Total number of visits: {}\".format(total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "frozen-convenience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Visit: integer (nullable = true)\n",
      " |-- Visits: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Dx10_prin: string (nullable = true)\n",
      " |-- Dx10_1: string (nullable = true)\n",
      " |-- Dx10_2: string (nullable = true)\n",
      " |-- Dx10_3: string (nullable = true)\n",
      " |-- Dx10_4: string (nullable = true)\n",
      " |-- Dx10_5: string (nullable = true)\n",
      " |-- Dx10_6: string (nullable = true)\n",
      " |-- Dx10_7: string (nullable = true)\n",
      " |-- Dx10_8: string (nullable = true)\n",
      " |-- Dx10_9: string (nullable = true)\n",
      " |-- Dx10_10: string (nullable = true)\n",
      " |-- Dx10_11: string (nullable = true)\n",
      " |-- Dx10_12: string (nullable = true)\n",
      " |-- Dx10_13: string (nullable = true)\n",
      " |-- Dx10_14: string (nullable = true)\n",
      " |-- Dx10_15: string (nullable = true)\n",
      " |-- Dx10_16: string (nullable = true)\n",
      " |-- Dx10_17: string (nullable = true)\n",
      " |-- Dx10_18: string (nullable = true)\n",
      " |-- Dx10_19: string (nullable = true)\n",
      " |-- Dx10_20: string (nullable = true)\n",
      " |-- Dx10_21: string (nullable = true)\n",
      " |-- Dx10_22: string (nullable = true)\n",
      " |-- Dx10_23: string (nullable = true)\n",
      " |-- Dx10_24: string (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(\"next_Date\", \"next_Id\", \"decFirst\", \"daysUntilDecFirst\", \"datediff\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "generous-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTS NUMBER OF VISITS FILTERED OUT:\n",
    "# df.filter(f.col(\"daysUntilDecFirst\") < 0).select(countDistinct(\"ID\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-precipitation",
   "metadata": {},
   "source": [
    "Visits to keep: 27502639\n",
    "\n",
    "Visits filtered out: 475293\n",
    "\n",
    "Total visits: 27977932"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-garage",
   "metadata": {},
   "source": [
    "## Shuffle the data at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "included-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "df = df.orderBy(rand(seed=1234))\n",
    "df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "assert df.count()== total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-software",
   "metadata": {},
   "source": [
    "## Create an 80-20 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "tribal-reader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 27502638\n",
      "Splitting at: 22002110\n"
     ]
    }
   ],
   "source": [
    "split = int(total_count * 0.8)\n",
    "print(\"Total count: {}\".format(total_count))\n",
    "print(\"Splitting at: {}\".format(split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "otherwise-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.filter(col(\"index\").between(0, split)).drop(\"index\") # repartitin or coalesce after filtering -- 1GB per partition rule\n",
    "test = df.filter(col(\"index\").between(split + 1, total_count)).drop(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eligible-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 22002111\n",
      "Test size: 5500527\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size: {}\".format(train.count()))\n",
    "print(\"Test size: {}\".format(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-lover",
   "metadata": {},
   "source": [
    "## Save train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "blond-cisco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train file took 165.25227093696594 seconds\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "train.write.format('csv').option('header', True).option('sep',',').mode('overwrite').save(dirPath + 'train')\n",
    "end = time.time()\n",
    "print(\"Saving train file took {} seconds\".format(end - begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "absent-start",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving test file took 117.19075202941895 seconds\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "test.write.format('csv').option('header', True).option('sep',',').mode('overwrite').save(dirPath + 'test')\n",
    "end = time.time()\n",
    "print(\"Saving test file took {} seconds\".format(end - begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-uruguay",
   "metadata": {},
   "source": [
    "## Load from saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ranking-villa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell took 2.823918581008911 seconds\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "train_loaded = spark.read.format(\"csv\").load(dirPath + \"train\",\n",
    "                     sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "end = time.time()\n",
    "print(\"Cell took {} seconds\".format(end - begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "danish-nature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell took 0.7571585178375244 seconds\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "test_loaded = spark.read.load(dirPath + \"test\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "end = time.time()\n",
    "print(\"Cell took {} seconds\".format(end - begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-mailing",
   "metadata": {},
   "source": [
    "## Assert no errors during saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "assumed-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train.count() == train_loaded.count()\n",
    "assert test.count() == test_loaded.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-wilderness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
