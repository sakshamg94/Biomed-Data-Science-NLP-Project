{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "balanced-command",
   "metadata": {},
   "source": [
    "# Prepare Data For Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "connected-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import csv\n",
    "import collections\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hired-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stupid-passenger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'35G'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the session\n",
    "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '35G')\n",
    "        .set('spark.driver.memory', '35G')\n",
    "        .set('spark.driver.maxResultSize', '35G'))\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "# arrow enabling is what makes the conversion from pandas to spark dataframe really fast\n",
    "sc._conf.get('spark.driver.memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "figured-denial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-50-244.us-west-2.compute.internal:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8aadffa4d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-headset",
   "metadata": {},
   "source": [
    "# Step 1: Read word2vec learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "developing-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, Word2Vec, Word2VecModel, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "dirPath = '/home/ubuntu/BioMedProject/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brave-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_LENGTH = 256 # 16, 32, 64, 128, 256\n",
    "\n",
    "embedFilePath = dirPath + \"W2V_Models/w2v_ICD_embed_{}.model\".format(EMBED_LENGTH)\n",
    "\n",
    "loaded_model = Word2VecModel.load(embedFilePath)\n",
    "df_embeddings = loaded_model.getVectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-reputation",
   "metadata": {},
   "source": [
    "# Step 2: Read the train and test data with ALL patient visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "attempted-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCorpusFilename = dirPath + \"ModelData/trainCorpusAllVisits.pkl\"\n",
    "testCorpusFilename = dirPath + \"ModelData/testCorpusAllVisits.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dutch-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in train/test datasets\n",
    "train = spark.read.load(dirPath + \"train\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "test = spark.read.load(dirPath + \"test\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-model",
   "metadata": {},
   "source": [
    "## Step 2.1 SKIP TO NEXT SECTION : The following 2 cells are a 1 time job and have been run-saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "patient-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Filter for columns with ICD Codes and convert to Pandas DF\n",
    "trainICDCodes = train.select(train.columns[7:32]).toPandas()\n",
    "testICDCodes = test.select(test.columns[7:32]).toPandas()\n",
    "\n",
    "# 3. Convert Pandas to a list of lists\n",
    "trainCorpus = [[elem[:3] for elem in row if type(elem) == str] for row in trainICDCodes.values.tolist()]\n",
    "testCorpus = [[elem[:3] for elem in row if type(elem) == str] for row in testICDCodes.values.tolist()]\n",
    "\n",
    "# 4. Convert list ICD codes to strings\n",
    "sentence = \" \"\n",
    "trainCorpus = [sentence.join(trainCorpus[i]) for i in range(len(trainCorpus))]\n",
    "testCorpus = [sentence.join(testCorpus[i]) for i in range(len(testCorpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "exact-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save train and test \n",
    "with open(trainCorpusFilename, 'wb') as handle:\n",
    "    pickle.dump(trainCorpus, handle)\n",
    "    \n",
    "with open(testCorpusFilename, 'wb') as handle:\n",
    "    pickle.dump(testCorpus, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cosmetic-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainCorpus\n",
    "del testCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-nigeria",
   "metadata": {},
   "source": [
    "## Step 2.2 Load the saved train and test data with all visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "direct-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(trainCorpusFilename, 'rb') as handle:\n",
    "    corpus_train = pickle.load(handle)\n",
    "    \n",
    "with open(testCorpusFilename, 'rb') as handle:\n",
    "    corpus_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "final-harris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.002111 million TRAIN visits\n",
      "5.500527 million TEST visits\n"
     ]
    }
   ],
   "source": [
    "print(\"{} million TRAIN visits\".format(len(corpus_train)/1e6))\n",
    "print(\"{} million TEST visits\".format(len(corpus_test)/1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-serve",
   "metadata": {},
   "source": [
    "# Step 3: Make transformations on the train and test dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-computer",
   "metadata": {},
   "source": [
    "## Step 3.1 Transformations for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "electronic-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus_train = pd.DataFrame(corpus_train, columns = ['sentences'])\n",
    "df_corpus_train = spark.createDataFrame(df_corpus_train)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentences\", outputCol=\"tokens\")\n",
    "tokenized_corpus_train = tokenizer.transform(df_corpus_train).select(\"tokens\")\n",
    "\n",
    "res_train = loaded_model.transform(tokenized_corpus_train)\n",
    "to_array = F.udf(lambda v: v.toArray().tolist(), T.ArrayType(T.FloatType()))\n",
    "res_train = res_train.withColumn('features', to_array('features'))\n",
    "\n",
    "X_train = res_train.select([F.col(\"features\")[i] for i in range(EMBED_LENGTH)])\n",
    "\n",
    "train = train.select(\"ID\", \"Visit\", \"Visits\", \"Age\", \"Sex\", \"Race\", \"Label\")\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_NUMERIC\").fit(train) for column in ['Sex', 'Race']]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "train = pipeline.fit(train).transform(train)\n",
    "train = train.drop(\"Sex\", \"Race\")\n",
    "\n",
    "# For each column (Sex and Race), we convert each string\n",
    "# to some double. I.e. for Seame transformation is applied to tx, \"M\" = 0.0 and \"F\" = 1.0.\n",
    "# The she Race column\n",
    "# We remove the original \"Sex\" and \"Race\" columns which\n",
    "# contain the string versions.\n",
    "# Converts Race and Sex column types from double to integer\n",
    "train = train.withColumn(\"Sex\", train[\"Sex_NUMERIC\"].cast(T.IntegerType()))\n",
    "train = train.withColumn(\"Race\", train[\"Race_NUMERIC\"].cast(T.IntegerType()))\n",
    "train = train.drop(\"Sex_NUMERIC\", \"Race_NUMERIC\")\n",
    "# Groups ages together within the same 5 year window i.e. (65 - 69) = 65 or (25-29) = 25 -- \n",
    "# integer encode Age rather than one-hot encode which works best for categorical variables. \n",
    "# Treat age as numeric\n",
    "# Turns age column into integers\n",
    "train = train.withColumn(\"Age\", (5/100*round(train[\"Age\"] / 5 )).cast(T.FloatType()))\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"Sex\", \"Race\"], outputCols=[\"Sex_Encoded\", \"Race_Encoded\"])\n",
    "model_train = encoder.fit(train)\n",
    "train = model_train.transform(train)\n",
    "\n",
    "train = train.drop(\"Sex\", \"Race\")\n",
    "train = train.select(\"Sex_Encoded\", \"Race_Encoded\", \"Age\", \"Label\")\n",
    "w=Window.orderBy(lit(1))\n",
    "X_train = X_train.withColumn(\"id\",  row_number().over(w))\n",
    "train  =train.withColumn(\"id\", row_number().over(w))\n",
    "master_df_train = X_train.join(train, \"id\",\"outer\")\n",
    "master_df_train = master_df_train.drop(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-planet",
   "metadata": {},
   "source": [
    "## Step 3.2: Transformations for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hollywood-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus_test = pd.DataFrame(corpus_test, columns = ['sentences'])\n",
    "df_corpus_test = spark.createDataFrame(df_corpus_test)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentences\", outputCol=\"tokens\")\n",
    "tokenized_corpus_test = tokenizer.transform(df_corpus_test).select(\"tokens\")\n",
    "\n",
    "res_test = loaded_model.transform(tokenized_corpus_test)\n",
    "\n",
    "to_array = F.udf(lambda v: v.toArray().tolist(), T.ArrayType(T.FloatType()))\n",
    "res_test = res_test.withColumn('features', to_array('features'))\n",
    "\n",
    "X_test = res_test.select([F.col(\"features\")[i] for i in range(EMBED_LENGTH)])\n",
    "\n",
    "test = test.select(\"ID\", \"Visit\", \"Visits\", \"Age\", \"Sex\", \"Race\", \"Label\")\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_NUMERIC\").fit(test) for column in ['Sex', 'Race']]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "test = pipeline.fit(test).transform(test)\n",
    "test = test.drop(\"Sex\", \"Race\")\n",
    "\n",
    "# Converts Race and Sex column types from double to integer\n",
    "test = test.withColumn(\"Sex\", test[\"Sex_NUMERIC\"].cast(T.IntegerType()))\n",
    "test = test.withColumn(\"Race\", test[\"Race_NUMERIC\"].cast(T.IntegerType()))\n",
    "test = test.drop(\"Sex_NUMERIC\", \"Race_NUMERIC\")\n",
    "test = test.withColumn(\"Age\", (5/100*round(test[\"Age\"] / 5)).cast(T.FloatType()))\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"Sex\", \"Race\"], outputCols=[\"Sex_Encoded\", \"Race_Encoded\"])\n",
    "model_test = encoder.fit(test)\n",
    "test = model_test.transform(test)\n",
    "\n",
    "test = test.drop(\"Sex\", \"Race\")\n",
    "test = test.select(\"Sex_Encoded\", \"Race_Encoded\", \"Age\", \"Label\")\n",
    "w=Window.orderBy(lit(1))\n",
    "X_test = X_test.withColumn(\"id\",  row_number().over(w))\n",
    "test = test.withColumn(\"id\", row_number().over(w))\n",
    "master_df_test = X_test.join(test, \"id\",\"outer\")\n",
    "master_df_test = master_df_test.drop(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-broad",
   "metadata": {},
   "source": [
    "# Step 4: Collate the data into Pyspark-able format "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-partition",
   "metadata": {},
   "source": [
    "## Make the data ready for some baseline binary classifiers like Random Forest, Logistic regression, Gradient Boosted Decision Trees etc. See [here](https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "incorporate-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = ['features[{}]'.format(i) for i in range(EMBED_LENGTH)] + [\"Sex_Encoded\", \"Race_Encoded\", \"Age\"]\n",
    "\n",
    "# # for 0 embedding length -- use only demographic columns\n",
    "# master_df = train\n",
    "# inputCols = [\"Sex_Encoded\", \"Race_Encoded\", \"Age\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=inputCols, outputCol=\"features\") # assemble ALL the features in a vector \n",
    "stages = [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "incredible-machinery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel_train = pipeline.fit(master_df_train)\n",
    "fitting_ready_df_train = pipelineModel_train.transform(master_df_train)\n",
    "fitting_ready_df_train = fitting_ready_df_train.select(['Label', 'features'])\n",
    "fitting_ready_df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "offensive-effects",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel_test = pipeline.fit(master_df_test)\n",
    "fitting_ready_df_test = pipelineModel_test.transform(master_df_test)\n",
    "fitting_ready_df_test = fitting_ready_df_test.select(['Label', 'features'])\n",
    "fitting_ready_df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "answering-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = None\n",
    "test = None\n",
    "master_df_train = None\n",
    "master_df_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "saved-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelReadyTrainFilename = dirPath + f'ModelData/modelReadyTrain_{EMBED_LENGTH}.parquet'\n",
    "modelReadyTestFilename = dirPath + f'ModelData/modelReadyTest_{EMBED_LENGTH}.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "industrial-surname",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving took 5.642933615048727 minutes\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "fitting_ready_df_test.write.mode(\"overwrite\").save(modelReadyTestFilename, format=\"parquet\")\n",
    "t2 = time.time()\n",
    "print(\"Saving took {} minutes\".format((t2-t1)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "everyday-protection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving took 26.053846804300942 minutes\n"
     ]
    }
   ],
   "source": [
    "# 5. Save final dataframes\n",
    "t1 = time.time()\n",
    "fitting_ready_df_train.write.mode(\"overwrite\").save(modelReadyTrainFilename, format=\"parquet\")\n",
    "t2 = time.time()\n",
    "print(\"Saving took {} minutes\".format((t2-t1)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-mobile",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
