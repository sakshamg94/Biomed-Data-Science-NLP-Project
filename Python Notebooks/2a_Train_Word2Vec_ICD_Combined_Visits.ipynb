{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Embeddings - Using PySpark and PyArrow (distributed ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialise spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-50-244.us-west-2.compute.internal:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4fff839250>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the session\n",
    "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '35G')\n",
    "        .set('spark.driver.memory', '35G')\n",
    "        .set('spark.driver.maxResultSize', '35G'))\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.config('spark.sql.shuffle.partitions',300).getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "# arrow enabling is what makes the conversion from pandas to spark dataframe really fast\n",
    "sc._conf.get('spark.driver.memory')\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirPath = '/home/ubuntu/BioMedProject/Data/'\n",
    "trainCorpusFilename = dirPath + \"w2v_training_Corpus_ICD_combined_visits.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Convert raw data to `CORPUS` (skip if already saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in train dataset\n",
    "train_df = spark.read.load(dirPath + \"train\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "train_df = train_df.select(train_df.colRegex(\"`[IDx_\\dprin]+`\")) # Selects ID + ICD Code columns\n",
    "\n",
    "# 2. Filter for columns with ICD Codes and convert to Pandas DF\n",
    "trainICDCodes = train_df.select(train_df.columns[1:]).toPandas()\n",
    "\n",
    "# 3. Convert Pandas to a list of lists\n",
    "trainCorpus = [[elem[:3] for elem in row if type(elem) == str] for row in trainICDCodes.values.tolist()]\n",
    "\n",
    "# 4. Create list of patient IDs\n",
    "patientIDs = train_df.select(train_df.columns[0]).toPandas()\n",
    "patientIDs = [row[0] for row in patientIDs.values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0.0 million visits\n",
      "Completed 5.0 million visits\n",
      "Completed 10.0 million visits\n",
      "Completed 15.0 million visits\n",
      "Completed 20.0 million visits\n",
      "Completed 22.002111 million visits\n"
     ]
    }
   ],
   "source": [
    "# 5. Generate dict mapping of patient_id -> all ICD codes across all visits\n",
    "visits = {}\n",
    "for idx in range(len(patientIDs)):\n",
    "    patient_id = patientIDs[idx]\n",
    "    codes = trainCorpus[idx]\n",
    "    \n",
    "    if patient_id in visits:\n",
    "        old_codes = visits[patient_id]\n",
    "        codes.extend(old_codes)\n",
    "    \n",
    "    visits[patient_id] = codes\n",
    "    \n",
    "    if idx % 5e6 == 0:\n",
    "        print(f\"Completed {idx / 1e6} million visits\")\n",
    "\n",
    "print(f\"Completed {len(patientIDs) / 1e6} million visits\")\n",
    "assert(len(visits.keys()) == len(np.unique(trainID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Generate corpus: list of list of codes for each patient\n",
    "corpus = []\n",
    "for key in visits.keys():\n",
    "    corpus.append(visits[key])\n",
    "\n",
    "# 7. Generate trainCorpus: list of sentences where each sentence is a space-delimited string of icd codes\n",
    "sentence = \" \"\n",
    "trainCorpus = [sentence.join(corpus[i]) for i in range(len(corpus))]\n",
    "\n",
    "# 8. Save train\n",
    "with open(trainCorpusFilename, 'wb') as handle:\n",
    "    pickle.dump(trainCorpus, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load the train `CORPUS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           sentences|\n",
      "+--------------------+\n",
      "|     L30 R21 R50 Z00|\n",
      "|Z46 R33 N39 F17 R...|\n",
      "|I87 R60 I10 G40 Z...|\n",
      "|I50 I12 N18 E78 Z...|\n",
      "|R51 G89 R07 R51 R...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "4.502821 million train sentences\n"
     ]
    }
   ],
   "source": [
    "# 1. Load data from pickle file\n",
    "with open(trainCorpusFilename, 'rb') as handle:\n",
    "    trainCorpus = pickle.load(handle)\n",
    "    \n",
    "# 2. Create Pandas DataFrame\n",
    "trainDf = pd.DataFrame(trainCorpus, columns = ['sentences'])\n",
    "del trainCorpus\n",
    "\n",
    "# 3. Convert Pandas to PySpark\n",
    "trainSparkDF = spark.createDataFrame(trainDf)\n",
    "\n",
    "# 4. Visualize sentences\n",
    "trainSparkDF.show(5)\n",
    "print(\"{} million train sentences\".format(trainSparkDF.count() / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import Word2Vec, Word2VecModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "EMBED_LENGTHS = [64, 128, 256] # 16, 32\n",
    "\n",
    "\n",
    "for embed in EMBED_LENGTHS:\n",
    "    # 1. Tokenize sentences\n",
    "    tokenizer = Tokenizer(inputCol=\"sentences\", outputCol=\"tokens\")\n",
    "    tokenized = tokenizer.transform(trainSparkDF).select(\"tokens\")\n",
    "\n",
    "    #2. Initialize Word2Vec Model\n",
    "    w2v = Word2Vec(vectorSize=embed, minCount=1, inputCol=\"tokens\", outputCol=\"features\")\\\n",
    "                .setSeed(1234).setWindowSize(25)\n",
    "\n",
    "    # 3. Train Word2Vec Model\n",
    "    start = time.time()\n",
    "    word2vec_model = w2v.fit(tokenized)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"Time taken: {(end - start)/60} mins for embed length {embed}\")\n",
    "    times.append((end - start)/60)\n",
    "\n",
    "    path = dirPath + f\"W2V_Models/w2v_ICD_Combined_Visits_embed_{embed}.model\"\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    word2vec_model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47.76017210483551, 70.53435916105906, 125.14494510094325, 236.3829428911209]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Embeddings Tabular form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec, Word2VecModel\n",
    "EMBED_LENGTH = 256\n",
    "dirPath = '/home/ubuntu/BioMedProject/Data/'\n",
    "embedFilePath = dirPath + f\"W2V_Models/w2v_ICD_Combined_Visits_embed_{EMBED_LENGTH}.model\"\n",
    "loaded_model = Word2VecModel.load(embedFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/pyspark/sql/pandas/conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>041</td>\n",
       "      <td>[-0.02264483831822872, 0.02762742154300213, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>070</td>\n",
       "      <td>[0.005771160125732422, 0.004969617817550898, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>129</td>\n",
       "      <td>[0.018249496817588806, 0.048011571168899536, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>148</td>\n",
       "      <td>[0.011404353193938732, 0.011679872870445251, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>151</td>\n",
       "      <td>[0.06326667219400406, -0.12140163034200668, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>z95</td>\n",
       "      <td>[-0.06973341852426529, -0.006060560699552298, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>z96</td>\n",
       "      <td>[0.043614502996206284, 0.05406169965863228, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>z97</td>\n",
       "      <td>[-0.2829343378543854, -0.18949846923351288, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>z98</td>\n",
       "      <td>[-0.18805629014968872, -0.08867216110229492, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>z99</td>\n",
       "      <td>[0.16139481961727142, 0.07389330863952637, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1651 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word                                             vector\n",
       "89    041  [-0.02264483831822872, 0.02762742154300213, -0...\n",
       "197   070  [0.005771160125732422, 0.004969617817550898, 0...\n",
       "1432  129  [0.018249496817588806, 0.048011571168899536, -...\n",
       "794   148  [0.011404353193938732, 0.011679872870445251, -...\n",
       "1037  151  [0.06326667219400406, -0.12140163034200668, 0....\n",
       "...   ...                                                ...\n",
       "1528  z95  [-0.06973341852426529, -0.006060560699552298, ...\n",
       "1172  z96  [0.043614502996206284, 0.05406169965863228, 0....\n",
       "676   z97  [-0.2829343378543854, -0.18949846923351288, -0...\n",
       "995   z98  [-0.18805629014968872, -0.08867216110229492, 0...\n",
       "1429  z99  [0.16139481961727142, 0.07389330863952637, -0....\n",
       "\n",
       "[1651 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embed = loaded_model.getVectors().toPandas()\n",
    "df_embed = df_embed.sort_values(\"word\")\n",
    "df_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = open(f\"/home/ubuntu/BioMedProject/embed_{EMBED_LENGTH}.csv\", \"w\")\n",
    "\n",
    "for word, embed in df_embed.values:\n",
    "    csvFile.write(word)\n",
    "    for val in embed.toArray().tolist():\n",
    "        csvFile.write(f\",{val}\")\n",
    "    csvFile.write(\"\\n\")\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
