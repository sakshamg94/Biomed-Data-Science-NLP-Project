{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Embeddings - Using PySpark and PyArrow (distributed ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# !pip install pyspark\n",
    "# !pip install -U -q PyDrive\n",
    "# !sudo apt install openjdk-8-jdk-headless -qq\n",
    "# !pip install pyarrow\n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialise spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-50-244.us-west-2.compute.internal:4051\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f314ead1fd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the session\n",
    "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '35G')\n",
    "        .set('spark.driver.memory', '35G')\n",
    "        .set('spark.driver.maxResultSize', '35G'))\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "# arrow enabling is what makes the conversion from pandas to spark dataframe really fast\n",
    "sc._conf.get('spark.driver.memory')\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirPath = '/home/ubuntu/BioMedProject/Data/'\n",
    "trainCorpusFilename = dirPath + \"w2v_training_Corpus_ICD.pkl\"\n",
    "testCorpusFilename = dirPath + \"w2v_testing_Corpus_iCD.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Convert raw data to `CORPUS` (skip if already saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in train/test datasets\n",
    "train = spark.read.load(dirPath + \"train\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "test = spark.read.load(dirPath + \"test\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# 2. Filter for columns with ICD Codes and convert to Pandas DF\n",
    "trainICDCodes = train.select(train.columns[7:32]).toPandas()\n",
    "testICDCodes = test.select(test.columns[7:32]).toPandas()\n",
    "\n",
    "# 3. Convert Pandas to a list of lists\n",
    "trainCorpus = [[elem[:3] for elem in row if type(elem) == str] for row in trainICDCodes.values.tolist()]\n",
    "testCorpus = [[elem[:3] for elem in row if type(elem) == str] for row in testICDCodes.values.tolist()]\n",
    "\n",
    "# 4. Remove 'sentences' (or visits) with only 1 ICD code\n",
    "trainCorpus = [sentence for sentence in trainCorpus if len(sentence) > 1]\n",
    "testCorpus = [sentence for sentence in testCorpus if len(sentence) > 1]\n",
    "\n",
    "# 5. Convert list ICD codes to strings\n",
    "sentence = \" \"\n",
    "trainCorpus = [sentence.join(trainCorpus[i]) for i in range(len(trainCorpus))]\n",
    "testCorpus = [sentence.join(testCorpus[i]) for i in range(len(testCorpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save train and test \n",
    "with open(trainCorpusFilename, 'wb') as handle:\n",
    "    pickle.dump(trainCorpus, handle)\n",
    "    \n",
    "with open(testCorpusFilename, 'wb') as handle:\n",
    "    pickle.dump(testCorpus, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load the train `CORPUS` (which excludes visits with exactly 1 ICD code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           sentences|\n",
      "+--------------------+\n",
      "| T83 R82 N40 R33 F17|\n",
      "|     F10 E11 I10 I63|\n",
      "|J69 E46 N17 I48 I...|\n",
      "|             T24 T31|\n",
      "|             M54 M79|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "18.283298 million train sentences\n"
     ]
    }
   ],
   "source": [
    "# 1. Load data from pickle file\n",
    "with open(trainCorpusFilename, 'rb') as handle:\n",
    "    trainCorpus = pickle.load(handle)\n",
    "    \n",
    "# 2. Create Pandas DataFrame\n",
    "trainDf = pd.DataFrame(trainCorpus, columns = ['sentences'])\n",
    "del trainCorpus\n",
    "\n",
    "# 3. Convert Pandas to PySpark\n",
    "trainSparkDF = spark.createDataFrame(trainDf)\n",
    "\n",
    "# 4. Visualize sentences\n",
    "trainSparkDF.show(5)\n",
    "print(\"{} million train sentences\".format(trainSparkDF.count() / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import Word2Vec, Word2VecModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "EMBED_LENGTH = 16\n",
    "\n",
    "# 1. Tokenize sentences\n",
    "tokenizer = Tokenizer(inputCol=\"sentences\", outputCol=\"tokens\")\n",
    "tokenized = tokenizer.transform(trainSparkDF).select(\"tokens\")\n",
    "\n",
    "#2. Initialize Word2Vec Model\n",
    "w2v = Word2Vec(vectorSize=EMBED_LENGTH, minCount=1, inputCol=\"tokens\", outputCol=\"features\")\\\n",
    "            .setSeed(1234).setWindowSize(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 13.968408679962158 mins\n"
     ]
    }
   ],
   "source": [
    "# 3. Train Word2Vec Model\n",
    "start = time.time()\n",
    "word2vec_model = w2v.fit(tokenized)\n",
    "end = time.time()\n",
    "print(\"Time taken: {} mins\".format((end - start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|word|              vector|\n",
      "+----+--------------------+\n",
      "| k40|[-0.0542779192328...|\n",
      "| f25|[-0.2230365127325...|\n",
      "| k46|[-0.2039844840764...|\n",
      "| j67|[-0.1282166838645...|\n",
      "| g63|[0.00770874321460...|\n",
      "| a28|[-0.0551535189151...|\n",
      "| i82|[-0.4175004065036...|\n",
      "| q25|[0.25801816582679...|\n",
      "| g65|[-0.0121308276429...|\n",
      "| c55|[-0.6497572064399...|\n",
      "| k65|[-0.7179201841354...|\n",
      "| d74|[-0.2180854082107...|\n",
      "| c71|[-0.0597165934741...|\n",
      "| p58|[0.15644863247871...|\n",
      "| n07|[0.16744512319564...|\n",
      "| h82|[0.44940406084060...|\n",
      "| k03|[-0.5938100814819...|\n",
      "| m96|[0.03103973902761...|\n",
      "| b47|[-0.1402707397937...|\n",
      "| o46|[0.08599423617124...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec_model.getVectors().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dirPath + \"W2V_Models/w2v_ICD_embed_{}.model\".format(EMBED_LENGTH)\n",
    "if os.path.exists(path):\n",
    "    os.remove(path)\n",
    "word2vec_model.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Testing: transform vectors of choice using the trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Word2VecModel.load(path)\n",
    "word2vec_df = loaded_model.getVectors()\n",
    "word2vec_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame([\"prg028 prv028 prv028\", \"prv028\"], columns = ['sentences'])\n",
    "testing_sparkDF =spark.createDataFrame(test_df)\n",
    "testing_sparkDF = tokenizer.transform(testing_sparkDF).select(\"tokens\")\n",
    "testing_sparkDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "res = loaded_model.transform(testing_sparkDF)\n",
    "print(\"time taken : {}s\".format(time.time() - tic))\n",
    "res.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.toPandas().head().loc[:,\"features\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
